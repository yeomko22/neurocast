{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "r_lambda = 40\n",
    "nf = 200\n",
    "alpha = 40\n",
    "\n",
    "# original rating matrix\n",
    "R = np.array([[2, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0],\n",
    "              [3, 3, 4, 0, 3, 0, 0, 2, 2, 0, 0],\n",
    "              [5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 7, 0, 0, 5, 0],\n",
    "              [4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 5],\n",
    "              [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4],\n",
    "              [0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0],\n",
    "              [0, 0, 0, 3, 0, 0, 0, 0, 4, 5, 0]])\n",
    "print(R.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00937501 0.00894192 0.00111307 ... 0.00130772 0.00115214 0.00720873]\n",
      " [0.00146159 0.00896956 0.00942212 ... 0.00407354 0.00108615 0.0075778 ]\n",
      " [0.00901456 0.0038649  0.00020889 ... 0.00718687 0.00585541 0.0060983 ]\n",
      " ...\n",
      " [0.00685747 0.00700845 0.00460261 ... 0.00496061 0.00917623 0.00049663]\n",
      " [0.00191428 0.00637595 0.00815958 ... 0.0057553  0.0035395  0.00644017]\n",
      " [0.00663356 0.00381629 0.00871886 ... 0.00098702 0.00708245 0.00388322]]\n",
      "[[0.00897268 0.00284219 0.00820363 ... 0.00147741 0.00999528 0.00638416]\n",
      " [0.00065271 0.00873268 0.00225456 ... 0.00634664 0.00408266 0.00450254]\n",
      " [0.00505061 0.00334195 0.00893984 ... 0.00846426 0.00531104 0.00151123]\n",
      " ...\n",
      " [0.00366328 0.00984407 0.00095266 ... 0.00120807 0.00974001 0.00895036]\n",
      " [0.00789468 0.00019431 0.00742314 ... 0.00029259 0.00846265 0.00944247]\n",
      " [0.00962856 0.00511733 0.0015536  ... 0.00766279 0.00455305 0.00234931]]\n"
     ]
    }
   ],
   "source": [
    "# nf : dimension of latent factors\n",
    "# nu : number of users\n",
    "# ni : number of items\n",
    "\n",
    "nu = R.shape[0]\n",
    "ni = R.shape[1]\n",
    "\n",
    "# X, Y : latent matrix of users and items\n",
    "X = np.random.rand(nu, nf) * 0.01\n",
    "Y = np.random.rand(ni, nf) * 0.01\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 1 0]\n",
      " [1 1 1 0 1 0 0 1 1 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 1 0]\n",
      " [1 0 1 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1 0 0 1 0]\n",
      " [0 0 0 1 0 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Pui = 1 if Rui > 0\n",
    "# Pui = 0 if Rui = 0\n",
    "P = np.copy(R)\n",
    "P[P > 0] = 1\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 81   1   1 161 161   1   1   1   1   1   1]\n",
      " [  1   1   1   1   1   1   1   1   1   1  41]\n",
      " [  1   1   1   1   1   1   1  41   1 161   1]\n",
      " [121 121 161   1 121   1   1  81  81   1   1]\n",
      " [201 201 201   1   1   1   1   1   1   1   1]\n",
      " [  1   1   1   1   1   1 281   1   1 201   1]\n",
      " [161   1 161   1   1   1   1   1   1   1 201]\n",
      " [  1   1   1   1   1 161   1   1   1   1 161]\n",
      " [  1   1   1   1   1   1 201   1   1 201   1]\n",
      " [  1   1   1 121   1   1   1   1 161 201   1]]\n"
     ]
    }
   ],
   "source": [
    "# Cui = 1 + alpha * Rui\n",
    "C = 1 + alpha * R\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C : confidence matrix\n",
    "# P : predict matrix\n",
    "# X : user latent matrix\n",
    "# Y : item latent matrix\n",
    "# r_lambda : regularization lambda\n",
    "def confidence_loss(C, P, xTy, X, Y, r_lambda):\n",
    "    predict_error = np.square(P - xTy)\n",
    "    print('predict loss:', np.sum(predict_error))\n",
    "    confidence_error = np.sum(C * predict_error)\n",
    "    print('confidence loss:', confidence_error)\n",
    "    regularization = r_lambda * (np.sum(np.square(X)) + np.sum(np.square(Y)))\n",
    "    print('regularization: ', regularization)\n",
    "    return confidence_error + regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict loss: 23.25926034929978\n",
      "confidence loss: 90.49493283569099\n",
      "regularization:  705.3022907734701\n",
      "initial loss:  795.797223609161\n",
      "---------------------------------------\n",
      "predict loss: 23.27289375471135\n",
      "confidence loss: 90.51327734180576\n",
      "regularization:  705.2833161340757\n",
      "loss:  795.7965934758814\n",
      "---------------------------------------\n",
      "predict loss: 23.286054678402046\n",
      "confidence loss: 90.53095335312098\n",
      "regularization:  705.2650875611698\n",
      "loss:  795.7960409142908\n",
      "---------------------------------------\n",
      "predict loss: 23.298692447847934\n",
      "confidence loss: 90.54790273860365\n",
      "regularization:  705.247652322921\n",
      "loss:  795.7955550615246\n",
      "---------------------------------------\n",
      "predict loss: 23.31077926942078\n",
      "confidence loss: 90.56409563494508\n",
      "regularization:  705.2310312614192\n",
      "loss:  795.7951268963642\n",
      "---------------------------------------\n",
      "predict loss: 23.32230393653068\n",
      "confidence loss: 90.57952257054737\n",
      "regularization:  705.2152262717334\n",
      "loss:  795.7947488422808\n",
      "---------------------------------------\n",
      "predict loss: 23.333267119684276\n",
      "confidence loss: 90.59418858530752\n",
      "regularization:  705.2002258928177\n",
      "loss:  795.7944144781252\n",
      "---------------------------------------\n",
      "predict loss: 23.343677849525054\n",
      "confidence loss: 90.60810885127191\n",
      "regularization:  705.1860094707365\n",
      "loss:  795.7941183220084\n",
      "---------------------------------------\n",
      "predict loss: 23.35355089940229\n",
      "confidence loss: 90.62130542219208\n",
      "regularization:  705.1725502439036\n",
      "loss:  795.7938556660957\n",
      "---------------------------------------\n",
      "predict loss: 23.362904845841875\n",
      "confidence loss: 90.63380483196146\n",
      "regularization:  705.1598176156978\n",
      "loss:  795.7936224476592\n",
      "---------------------------------------\n",
      "predict loss: 23.37176063943177\n",
      "confidence loss: 90.64563633089674\n",
      "regularization:  705.1477788156791\n",
      "loss:  795.7934151465759\n",
      "---------------------------------------\n",
      "predict loss: 23.38014055950756\n",
      "confidence loss: 90.65683060069492\n",
      "regularization:  705.1364001018615\n",
      "loss:  795.7932307025563\n",
      "---------------------------------------\n",
      "predict loss: 23.38806745691474\n",
      "confidence loss: 90.6674188279763\n",
      "regularization:  705.1256476194408\n",
      "loss:  795.7930664474171\n",
      "---------------------------------------\n",
      "predict loss: 23.395564212492957\n",
      "confidence loss: 90.67743204580562\n",
      "regularization:  705.1154880032404\n",
      "loss:  795.7929200490461\n",
      "---------------------------------------\n",
      "predict loss: 23.40265335661578\n",
      "confidence loss: 90.68690067484518\n",
      "regularization:  705.1058887897705\n",
      "loss:  795.7927894646157\n",
      "---------------------------------------\n",
      "predict loss: 23.409356808513124\n",
      "confidence loss: 90.69585421262668\n",
      "regularization:  705.0968186885922\n",
      "loss:  795.7926729012189\n",
      "---------------------------------------\n",
      "predict loss: 23.4156957042478\n",
      "confidence loss: 90.70432103214173\n",
      "regularization:  705.0882477503984\n",
      "loss:  795.7925687825401\n",
      "---------------------------------------\n",
      "predict loss: 23.421690289899516\n",
      "confidence loss: 90.71232826057818\n",
      "regularization:  705.0801474599081\n",
      "loss:  795.7924757204862\n",
      "---------------------------------------\n",
      "predict loss: 23.42735986232752\n",
      "confidence loss: 90.71990171630016\n",
      "regularization:  705.0724907746297\n",
      "loss:  795.7923924909298\n",
      "---------------------------------------\n",
      "predict loss: 23.432722744285908\n",
      "confidence loss: 90.72706588766815\n",
      "regularization:  705.0652521252184\n",
      "loss:  795.7923180128865\n",
      "---------------------------------------\n",
      "predict loss: 23.437796283996494\n",
      "confidence loss: 90.73384394145232\n",
      "regularization:  705.0584073891333\n",
      "loss:  795.7922513305856\n",
      "---------------------------------------\n",
      "predict loss: 23.44259687180237\n",
      "confidence loss: 90.74025775172552\n",
      "regularization:  705.0519338462536\n",
      "loss:  795.7921915979791\n",
      "---------------------------------------\n",
      "predict loss: 23.447139968425738\n",
      "confidence loss: 90.74632794248909\n",
      "regularization:  705.0458101228398\n",
      "loss:  795.7921380653289\n",
      "---------------------------------------\n",
      "predict loss: 23.451440140787856\n",
      "confidence loss: 90.75207393906591\n",
      "regularization:  705.040016128495\n",
      "loss:  795.7920900675609\n",
      "---------------------------------------\n",
      "predict loss: 23.45551110242753\n",
      "confidence loss: 90.75751402463199\n",
      "regularization:  705.0345329895047\n",
      "loss:  795.7920470141366\n",
      "---------------------------------------\n",
      "predict loss: 23.459365756365525\n",
      "confidence loss: 90.76266539926361\n",
      "regularization:  705.0293429809553\n",
      "loss:  795.7920083802189\n",
      "---------------------------------------\n",
      "predict loss: 23.46301623886982\n",
      "confidence loss: 90.76754423962852\n",
      "regularization:  705.0244294593364\n",
      "loss:  795.7919736989649\n",
      "---------------------------------------\n",
      "predict loss: 23.466473963029618\n",
      "confidence loss: 90.77216575800573\n",
      "regularization:  705.0197767967702\n",
      "loss:  795.7919425547759\n",
      "---------------------------------------\n",
      "predict loss: 23.46974966138418\n",
      "confidence loss: 90.77654425974015\n",
      "regularization:  705.0153703176501\n",
      "loss:  795.7919145773902\n",
      "---------------------------------------\n",
      "predict loss: 23.47285342710157\n",
      "confidence loss: 90.7806931985366\n",
      "regularization:  705.0111962381577\n",
      "loss:  795.7918894366943\n",
      "---------------------------------------\n",
      "predict loss: 23.475794753386165\n",
      "confidence loss: 90.78462522923135\n",
      "regularization:  705.0072416089359\n",
      "loss:  795.7918668381673\n",
      "---------------------------------------\n",
      "[array([[ 0.89133715,  0.84459596,  0.86539029,  0.87376166,  0.84970234,\n",
      "         0.34639682,  0.50797739,  0.72207859,  0.82489557,  0.75143637,\n",
      "         0.61775409],\n",
      "       [ 0.3718061 ,  0.3229558 ,  0.37766725,  0.19588858,  0.24290727,\n",
      "         0.38287547, -0.09995894,  0.14900085,  0.16345553, -0.01154529,\n",
      "         0.46490072],\n",
      "       [ 0.4229894 ,  0.42663055,  0.39559275,  0.61263674,  0.52254506,\n",
      "        -0.0327118 ,  0.76357022,  0.55866868,  0.62011385,  0.86730329,\n",
      "         0.1125998 ],\n",
      "       [ 0.97317585,  0.91810909,  0.94707636,  0.9385613 ,  0.91569448,\n",
      "         0.41292838,  0.55044088,  0.78025756,  0.8866804 ,  0.81080571,\n",
      "         0.70322124],\n",
      "       [ 0.94562461,  0.87159739,  0.93207955,  0.77551139,  0.80385575,\n",
      "         0.56315153,  0.2238056 ,  0.62098412,  0.70915203,  0.47637267,\n",
      "         0.82751835],\n",
      "       [ 0.20933627,  0.23692893,  0.18069839,  0.50271385,  0.37853133,\n",
      "        -0.2125915 ,  0.90863118,  0.49515734,  0.53776129,  0.94734515,\n",
      "        -0.12454949],\n",
      "       [ 0.90384619,  0.81514655,  0.90106671,  0.64208315,  0.7018225 ,\n",
      "         0.68497712,  0.04271067,  0.50693652,  0.5728606 ,  0.27425764,\n",
      "         0.91776064],\n",
      "       [ 0.65149567,  0.55199242,  0.6696157 ,  0.27001065,  0.37562612,\n",
      "         0.78550517, -0.28976532,  0.19994178,  0.21015009, -0.1441865 ,\n",
      "         0.91301651],\n",
      "       [ 0.22020882,  0.24615622,  0.19188649,  0.50463205,  0.38371645,\n",
      "        -0.20028624,  0.89059139,  0.49415101,  0.53760594,  0.93300008,\n",
      "        -0.10951393],\n",
      "       [ 0.72285543,  0.70568662,  0.68979948,  0.85542187,  0.77971158,\n",
      "         0.11954555,  0.7707322 ,  0.73892707,  0.83550082,  0.96442179,\n",
      "         0.35555863]])]\n"
     ]
    }
   ],
   "source": [
    "def optimize_user(X, Y, C, P, nu, nf, r_lambda):\n",
    "    yT = np.transpose(Y)\n",
    "    for u in range(nu):\n",
    "        Cu = np.diag(C[u])\n",
    "        yT_Cu_y = np.matmul(np.matmul(yT, Cu), Y)\n",
    "        lI = np.dot(r_lambda, np.identity(nf))\n",
    "        yT_Cu_pu = np.matmul(np.matmul(yT, Cu), P[u])\n",
    "        X[u] = np.linalg.solve(yT_Cu_y + lI, yT_Cu_pu)\n",
    "\n",
    "def optimize_item(X, Y, C, P, ni, nf, r_lambda):\n",
    "    xT = np.transpose(X)\n",
    "    for i in range(ni):\n",
    "        Ci = np.diag(C[:, i])\n",
    "        xT_Ci_x = np.matmul(np.matmul(xT, Ci), X)\n",
    "        lI = np.dot(r_lambda, np.identity(nf))\n",
    "        xT_Ci_pi = np.matmul(np.matmul(xT, Ci), P[:, i])\n",
    "        Y[i] = np.linalg.solve(xT_Ci_x + lI, xT_Ci_pi)\n",
    "        \n",
    "        \n",
    "predict = np.matmul(X, np.transpose(Y))\n",
    "initial_loss = confidence_loss(C, P, predict, X, Y, r_lambda)\n",
    "print('initial loss: ', initial_loss)\n",
    "print('---------------------------------------')\n",
    "\n",
    "for i in range(30):\n",
    "    optimize_user(X, Y, C, P, nu, nf, r_lambda)\n",
    "    optimize_item(X, Y, C, P, ni, nf, r_lambda)\n",
    "    predict = np.matmul(X, np.transpose(Y))\n",
    "    loss = confidence_loss(C, P, predict, X, Y, r_lambda)\n",
    "    print('loss: ', loss)\n",
    "    print('---------------------------------------')\n",
    "predict = np.matmul(X, np.transpose(Y))\n",
    "print([predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
